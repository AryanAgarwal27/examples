{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_TRAIN_SIZE = 60000\n",
    "MNIST_TEST_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''Basic Pytorch CNN'''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x, T=1.0):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    '''Wraps a PyTorch CNN for the MNIST dataset within an sklearn template by defining \n",
    "    .fit(), .predict(), and .predict_proba() functions. This template enables the PyTorch\n",
    "    CNN to flexibly be used within the sklearn architecture -- meaning it can be passed into\n",
    "    functions like cross_val_predict as if it were an sklearn model. The confidentlearning library\n",
    "    requires that all models adhere to this basic sklearn template and thus, this class allows\n",
    "    a PyTorch CNN to be used in for learning with noisy labels among other things.'''\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size = 64,\n",
    "        epochs = 6,\n",
    "        log_interval = 50, # Set to None to not print\n",
    "        lr = 0.01,\n",
    "        momentum = 0.5,\n",
    "        no_cuda = False,\n",
    "        seed = 1,\n",
    "        test_batch_size = MNIST_TEST_SIZE,\n",
    "        loader = None, # Set to 'test' to force fit() and predict_proba() on test_set\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.log_interval = log_interval\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.no_cuda = no_cuda\n",
    "        self.seed = seed\n",
    "        self.test_batch_size = test_batch_size\n",
    "        \n",
    "        self.cuda = not self.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        # Instantiate PyTorch model\n",
    "        self.model = Net()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "            \n",
    "        self.loader_kwargs = {'num_workers': 1, 'pin_memory': True} if self.cuda else {}\n",
    "        self.loader = loader\n",
    "\n",
    "    def fit(self, train_idx, train_labels = None, sample_weight = None, loader = 'train'):\n",
    "        '''This function adheres to sklearn's \"fit(X, y)\" format for compatibility with scikit-learn. \n",
    "        ** All inputs should be numpy arrays, not pyTorch Tensors\n",
    "        train_idx is not X, but instead a list of indices for X (and y if train_labels is None).\n",
    "        This function is a member of the cnn class which will handle creation of X, y from\n",
    "        the train_idx via the train_loader.'''\n",
    "        if self.loader is not None:\n",
    "            loader = self.loader\n",
    "        if train_labels is not None and len(train_idx) != len(train_labels):\n",
    "            raise ValueError(\"Check that train_idx and train_labels are the same length.\")\n",
    "            \n",
    "        if sample_weight is not None:\n",
    "            if len(sample_weight) != len(train_labels):\n",
    "                raise ValueError(\"Check that train_labels and sample_weight are the same length.\")\n",
    "            class_weight = sample_weight[torch.np.unique(train_labels, return_index=True)[1]]\n",
    "            class_weight = torch.from_numpy(class_weight).float()\n",
    "            if self.cuda:\n",
    "                class_weight = class_weight.cuda()\n",
    "        else:\n",
    "            class_weight = None\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "            root = '../data', \n",
    "            train = (loader=='train'), \n",
    "            download = True,\n",
    "            transform = transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Use provided labels if not None, o.w. use MNIST dataset training labels\n",
    "        if train_labels is not None:\n",
    "            # Create sparse tensor of train_labels with (-1)s for labels not in train_idx.\n",
    "            # We avoid train_data[idx] because train_data may very large, i.e. image_net\n",
    "            sparse_labels = torch.np.zeros(MNIST_TRAIN_SIZE if loader == 'train' else MNIST_TEST_SIZE, dtype=int) - 1\n",
    "            sparse_labels[train_idx] = train_labels\n",
    "            train_dataset.train_labels = sparse_labels\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "#             sampler=SubsetRandomSampler(train_idx if train_idx is not None else range(MNIST_TRAIN_SIZE)),\n",
    "            sampler=SubsetRandomSampler(train_idx), \n",
    "            batch_size=self.batch_size, \n",
    "            **self.loader_kwargs\n",
    "        )\n",
    "            \n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "            \n",
    "        # Train for self.epochs epochs\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            \n",
    "            # Enable dropout and batch norm layers\n",
    "            self.model.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                if self.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = F.nll_loss(output, target, class_weight)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if self.log_interval is not None and batch_idx % self.log_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_idx),\n",
    "                        100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    \n",
    "    def predict(self, idx = None, loader = None):\n",
    "        # get the index of the max probability\n",
    "        probs = self.predict_proba(idx, loader)\n",
    "        return probs.argmax(axis=1)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, idx = None, loader = None):        \n",
    "        if self.loader is not None:\n",
    "            loader = self.loader\n",
    "        if loader is None:\n",
    "            is_test_idx = (len(idx) == MNIST_TEST_SIZE) and \\\n",
    "                (torch.np.array(idx) == torch.np.arange(MNIST_TEST_SIZE)).all()\n",
    "            loader = 'test' if is_test_idx else 'train'       \n",
    "        dataset = datasets.MNIST(\n",
    "            root = '../data', \n",
    "            train = (loader=='train'), \n",
    "            download = True,\n",
    "            transform = transforms.Compose(\n",
    "                [transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]                                        \n",
    "            )\n",
    "        )        \n",
    "        # Filter by idx\n",
    "        if idx is not None:\n",
    "            if loader == 'train' and len(idx) != MNIST_TRAIN_SIZE:\n",
    "                dataset.train_data = dataset.train_data[idx]\n",
    "                dataset.train_labels = dataset.train_labels[idx]\n",
    "            elif loader == 'test' and len(idx) != MNIST_TEST_SIZE:\n",
    "                dataset.test_data = dataset.test_data[idx]\n",
    "                dataset.test_labels = dataset.test_labels[idx]            \n",
    "        \n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset = dataset,\n",
    "            batch_size=self.batch_size if loader=='train' else self.test_batch_size, \n",
    "            **self.loader_kwargs\n",
    "        )\n",
    "        \n",
    "        # sets model.train(False) inactivating dropout and batch-norm layers\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Run forward pass on model to compute outputs\n",
    "        outputs = []\n",
    "        for data, _ in loader:\n",
    "            if self.cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data, volatile=True)\n",
    "            output = self.model(data)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Outputs are log_softmax (log probabilities)\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        # Convert to probabilities and return the numpy array of shape N x K\n",
    "        pred = torch.np.exp(outputs.data.numpy())\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        target = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])).test_labels.numpy()\n",
    "        \n",
    "        pred = self.predict(loader = 'test')\n",
    "        correct = torch.np.count_nonzero(pred == target)\n",
    "        \n",
    "        print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct, MNIST_TEST_SIZE,\n",
    "            100. * correct / MNIST_TEST_SIZE))        \n",
    "\n",
    "    \n",
    "    def test_deprecated(self):\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "            batch_size=self.test_batch_size, \n",
    "            shuffle=True, \n",
    "            **self.loader_kwargs\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if self.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = self.model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    y_train = datasets.MNIST('../data', train=True).train_labels.numpy()\n",
    "    y_test = datasets.MNIST('../data', train=False).test_labels.numpy()\n",
    "\n",
    "    cnn = CNN(epochs=10)\n",
    "\n",
    "    mod_val = 1\n",
    "    train_idx = torch.np.arange(MNIST_TRAIN_SIZE)[torch.np.arange(MNIST_TRAIN_SIZE) % mod_val == 0]\n",
    "    train_labels = y_train[torch.np.arange(MNIST_TRAIN_SIZE) % mod_val == 0]\n",
    "    sample_weight = torch.np.ones(len(train_labels)) /len(train_labels)\n",
    "    sample_weight[(train_labels == 9) | (train_labels == 8)] *= 5\n",
    "    sample_weight = sample_weight / sum(sample_weight)\n",
    "\n",
    "    cnn.fit(train_idx, train_labels, sample_weight)\n",
    "    # cnn.model = m\n",
    "    # m = cnn.model = m\n",
    "\n",
    "    # Test to make sure predict is working\n",
    "    assert((cnn.predict_proba(loader='test').argmax(axis=1) == cnn.predict(loader='test')).all())\n",
    "\n",
    "    cnn.test()\n",
    "    pred = cnn.predict(loader='test')\n",
    "    torch.np.bincount(y_test[pred == y_test]) / torch.np.bincount(y_test).astype(float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
