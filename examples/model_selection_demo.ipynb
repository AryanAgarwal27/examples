{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Tutorial\n",
    "\n",
    "This tutorial will show you the main hyper-parameters for LearningWithNoisyLabels. There are only three!\n",
    "\n",
    "1. `prune_method` : str (default: `'prune_by_noise_rate'`), Method used for pruning.\n",
    "    * Values: [`'prune_by_class'`, `'prune_by_noise_rate'`, or `'both'`]. \n",
    "    * `'prune_by_noise_rate'`: works by removing examples with *high probability* of being mislabeled for every non-diagonal in the prune_counts_matrix (see pruning.py).\n",
    "    * `'prune_by_class'`: works by removing the examples with *smallest probability* of belonging to their given class label for every class.\n",
    "    * `'both'`: Finds the examples satisfying (1) AND (2) and removes their set conjunction. \n",
    "\n",
    "\n",
    "2. `prune_count_method` : str (default `'inverse_nm_dot_s'`)\n",
    "    * Values: [`'inverse_nm_dot_s'` or `'calibrate_confident_joint'`]\n",
    "    * DO NOT USE 'calibrate_confident_joint' if you already know the noise matrix and will call .fit(noise_matrix = known_noise_matrix) or .fit(inverse_noise_matrix = known_inverse_noise_matrix) because 'calibrate_confident_joint' will estimate the noise without using this information.\n",
    "    * IN ALL OTHER CASES, We recommend always using 'calibrate_confident_joint' because it is faster and more robust when no noise matrix info is given.\n",
    "    * Determines the method used to estimate the counts of the joint P(s, y) that will be used to determine how many examples to prune for every class that are flipped to every other class, as follows:\n",
    "    \n",
    "    ```python\n",
    "    if prune_count_method == 'inverse_nm_dot_s':\n",
    "        # Matrix of counts(y=k and s=l)\n",
    "        prune_count_matrix = inverse_noise_matrix * s_counts \n",
    "    elif prune_count_method == 'calibrate_confident_joint':\n",
    "        # Calibrate so (first make sum to 1, then make sum to total number of examples)\n",
    "        prune_count_matrix = confident_joint.T / float(confident_joint.sum()) * len(s)\n",
    "      ```\n",
    "\n",
    "\n",
    "3. converge_latent_estimates : bool (Default: False)\n",
    "    * If true, forces numerical consistency of latent estimates. Each is estimated independently, but they are related mathematically with closed form  equivalences. This will iteratively enforce mathematically consistency.\n",
    "\n",
    "## This tutorial uses hypopt for faster hyper-optimization using a validation set (instead of slow cross validation).\n",
    "### `$ pip install hypopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2 and 3 compatibility\n",
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypopt.model_selection import GridSearch\n",
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "from cleanlab.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.noise_generation import generate_noisy_labels\n",
    "from cleanlab.util import print_noise_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear_dataset(n_classes = 3, n_samples = 300):\n",
    "    X, y = make_classification(n_samples = n_samples, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1, n_classes=n_classes)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'prune_method': ['prune_by_class', 'prune_by_noise_rate', 'both'],\n",
    "    'prune_count_method': ['inverse_nm_dot_s', 'calibrate_confident_joint'],\n",
    "    'converge_latent_estimates': [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sparsity of the noise matrix.\n",
    "frac_zero_noise_rates = 0.0 # Consider increasing to 0.5\n",
    "# A proxy for the fraction of labels that are correct.\n",
    "avg_trace = 0.65 # ~35% wrong labels. Increasing makes the problem easier.\n",
    "# Amount of data for each dataset.\n",
    "dataset_size = 250 # Try 250 or 400 to use less or more data.\n",
    "num_classes = 3\n",
    "\n",
    "ds = make_linear_dataset(n_classes=num_classes, n_samples=num_classes*dataset_size)\n",
    "X, y = ds\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " =========== \n",
      " Naive Bayes \n",
      " ===========\n",
      "\n",
      " Noise Matrix (aka Noisy Channel) P(s|y) of shape (3, 3)\n",
      " p(s|y)\ty=0\ty=1\ty=2\n",
      "\t---\t---\t---\n",
      "s=0 |\t0.52\t0.1\t0.34\n",
      "s=1 |\t0.2\t0.82\t0.05\n",
      "s=2 |\t0.28\t0.07\t0.61\n",
      "\tTrace(matrix) = 1.95\n",
      "\n",
      "Accuracy with default parameters: 0.67\n",
      "Accuracy with optimized parameters: 0.74\n",
      "\n",
      "Optimal parameter settings using Naive Bayes\n",
      "--------------------------------------------\n",
      "converge_latent_estimates : False\n",
      "prune_count_method : calibrate_confident_joint\n",
      "prune_method : prune_by_class\n",
      "\n",
      " =================== \n",
      " Logistic Regression \n",
      " ===================\n",
      "\n",
      " Noise Matrix (aka Noisy Channel) P(s|y) of shape (3, 3)\n",
      " p(s|y)\ty=0\ty=1\ty=2\n",
      "\t---\t---\t---\n",
      "s=0 |\t0.52\t0.1\t0.34\n",
      "s=1 |\t0.2\t0.82\t0.05\n",
      "s=2 |\t0.28\t0.07\t0.61\n",
      "\tTrace(matrix) = 1.95\n",
      "\n",
      "Accuracy with default parameters: 0.66\n",
      "Accuracy with optimized parameters: 0.73\n",
      "\n",
      "Optimal parameter settings using Logistic Regression\n",
      "----------------------------------------------------\n",
      "converge_latent_estimates : False\n",
      "prune_count_method : calibrate_confident_joint\n",
      "prune_method : prune_by_class\n"
     ]
    }
   ],
   "source": [
    "for name, clf in [\n",
    "    (\n",
    "        \"Naive Bayes\", \n",
    "        GaussianNB(),\n",
    "    ),\n",
    "    (\n",
    "        \"Logistic Regression\", \n",
    "         LogisticRegression(random_state=0, solver = 'lbfgs', multi_class = 'auto'),\n",
    "    ),\n",
    "]:\n",
    "    print(\"\\n\", \"=\"*len(name), \"\\n\", name, '\\n', \"=\"*len(name))\n",
    "    np.random.seed(seed=0)\n",
    "    clf_copy = copy.deepcopy(clf)\n",
    "    # Compute p(y=k), the ground truth class prior on the labels.\n",
    "    py = np.bincount(y_train) / float(len(y_train))\n",
    "    # Generate the noisy channel to characterize the label errors.\n",
    "    noise_matrix = generate_noise_matrix_from_trace(\n",
    "        K = num_classes,\n",
    "        trace = num_classes * avg_trace, \n",
    "        py = py,\n",
    "        frac_zero_noise_rates = frac_zero_noise_rates,\n",
    "    )\n",
    "    print_noise_matrix(noise_matrix)\n",
    "    # Create the noisy labels. This method is exact w.r.t. the noise_matrix.\n",
    "    y_train_with_errors = generate_noisy_labels(y_train, noise_matrix)\n",
    "    lnl_cv = GridSearch(\n",
    "        model = LearningWithNoisyLabels(clf), \n",
    "        param_grid = param_grid, \n",
    "        num_threads = 4,\n",
    "        seed = 0,\n",
    "    )\n",
    "    lnl_cv.fit(\n",
    "        X_train = X_train, \n",
    "        y_train = y_train_with_errors,\n",
    "        X_val = X_val,\n",
    "        y_val = y_val,\n",
    "        verbose = False,\n",
    "    )\n",
    "    # Also compute the test score with default parameters\n",
    "    clf_copy.fit(X_train, y_train_with_errors)\n",
    "    score_opt = lnl_cv.model.score(X_test, y_test)\n",
    "    score_default = clf_copy.score(X_test, y_test)\n",
    "    print(\"Accuracy with default parameters:\", np.round(score_default, 2))\n",
    "    print(\"Accuracy with optimized parameters:\", np.round(score_opt, 2))\n",
    "    print()\n",
    "    s = \"Optimal parameter settings using {}\".format(name)\n",
    "    print(s)\n",
    "    print(\"-\"*len(s))\n",
    "    for key in lnl_cv.best_params.keys():\n",
    "        print(key, \":\", lnl_cv.best_params[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
