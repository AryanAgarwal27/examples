{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports enhance Python2/3 compatibility.\n",
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For visualizing images of label errors\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# urllib2 for python2 and python3\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "# where imagenet dataset is located\n",
    "train_dir = '/datasets/datasets/imagenet/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up name mapping for ImageNet train data\n",
    "url = 'https://gist.githubusercontent.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57/'\n",
    "url += 'raw/aa66dd9dbf6b56649fa3fab83659b2acbf3cbfd1/map_clsloc.txt'\n",
    "with urlopen(url) as f:\n",
    "    lines = [x.decode('utf-8') for x in f.readlines()]    \n",
    "    nid2name = dict([(l.split(\" \")[0], l.split(\" \")[2][:-1]) for l in lines])\n",
    "    \n",
    "dataset = datasets.ImageFolder(train_dir)\n",
    "nid2idx = dataset.class_to_idx\n",
    "idx2nid = {v: k for k, v in nid2idx.items()}\n",
    "name2nid = {v: k for k, v in nid2name.items()}\n",
    "idx2name = {k: nid2name[v] for k, v in idx2nid.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the train set on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO CHANGE EXPERIMENT\n",
    "# pyx_file = 'imagenet_val_out.npy' # NO FINE TUNING\n",
    "pyx_file = 'imagenet__train__model_resnet50__pyx.npy' # trained from scratch with 10fold cv\n",
    "\n",
    "# where imagenet dataset is located\n",
    "train_dir = '/datasets/datasets/imagenet/train/'\n",
    "# Stored results directory\n",
    "pyx_dir = '/datasets/cgn/pyx/imagenet/'\n",
    "\n",
    "# Load in data\n",
    "pyx = np.load(pyx_dir + pyx_file)\n",
    "imgs, labels = [list(z) for  z in zip(*datasets.ImageFolder(train_dir).imgs)]\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj = cleanlab.latent_estimation.estimate_confident_joint_from_probabilities(labels, pyx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = cleanlab.latent_estimation.estimate_joint(cj, labels, pyx)\n",
    "joint_non_diag = joint - np.eye(len(joint)) * joint.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_non_diag = cj - np.eye(len(cj)) * cj.diagonal()\n",
    "largest_non_diag_raveled = np.argsort(cj_non_diag.ravel())[::-1]\n",
    "largest_non_diag = np.unravel_index(largest_non_diag_raveled, cj_non_diag.shape)\n",
    "largest_non_diag = list(zip(*(list(z) for z in largest_non_diag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks that joint correctly has rows that are p(s)\n",
    "assert(all(joint.sum(axis = 1) - np.bincount(labels) / len(labels) < 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'bighorn' in sorted diagonal of cj: 374\n",
      "Index of 'bighorn' in sorted diagonal of joint: 374\n",
      "Index of 'bighorn' in sorted most noisy classes in cj: 150\n",
      "Index of 'bighorn' in sorted most noisy classes in joint: 150\n",
      "Index of 'bighorn' in sorted most noisy true classes in cj: 325\n",
      "Index of 'bighorn' in sorted most noisy true classes in joint: 325\n",
      "Least confident class by diagonal of cj: English_foxhound 167\n",
      "Least confident class by diagonal of joint: English_foxhound 167\n",
      "Least confident class by max sum of row of non-diagonal elements of cj: water_jug 899\n",
      "Least confident class by max sum of column of non-diagonal elements of cj: maillot 638\n",
      "Largest noise rate: [('projectile', 744), ('missile', 657)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of '{}' in sorted diagonal of cj: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in cj.diagonal().argsort()].index(class_name))\n",
    "\n",
    "print(\"Index of '{}' in sorted diagonal of joint: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in joint.diagonal().argsort()].index(class_name))\n",
    "\n",
    "print(\"Index of '{}' in sorted most noisy classes in cj: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in np.argsort(cj_non_diag.sum(axis = 0))[::-1]].index(class_name))\n",
    "\n",
    "print(\"Index of '{}' in sorted most noisy classes in joint: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in np.argsort(joint_non_diag.sum(axis = 0))[::-1]].index(class_name))\n",
    "\n",
    "print(\"Index of '{}' in sorted most noisy true classes in cj: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in np.argsort(cj_non_diag.sum(axis = 1))[::-1]].index(class_name))\n",
    "\n",
    "print(\"Index of '{}' in sorted most noisy true classes in joint: \".format(class_name), end = \"\")\n",
    "print([nid2name[idx2nid[i]] for i in np.argsort(joint_non_diag.sum(axis = 1))[::-1]].index(class_name))\n",
    "\n",
    "idx = cj.diagonal().argmin()\n",
    "print(\"Least confident class by diagonal of cj:\", nid2name[idx2nid[idx]], idx)\n",
    "idx = joint.diagonal().argmin()\n",
    "print(\"Least confident class by diagonal of joint:\", nid2name[idx2nid[idx]], idx)\n",
    "idx = cj_non_diag.sum(axis = 0).argmax()\n",
    "print(\"Least confident class by max sum of row of non-diagonal elements of cj:\", nid2name[idx2nid[idx]], idx)\n",
    "idx = joint_non_diag.sum(axis = 1).argmax()\n",
    "print(\"Least confident class by max sum of column of non-diagonal elements of cj:\", nid2name[idx2nid[idx]], idx)\n",
    "print('Largest noise rate:', [(nid2name[idx2nid[z]], z) for z in largest_non_diag[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645 | n04008634 projectile | n03773504 missile\n",
      "539 | n04493381 tub | n02808440 bathtub\n",
      "476 | n02895154 breastplate | n03146219 cuirass\n",
      "437 | n01693334 green_lizard | n01682714 American_chameleon\n",
      "435 | n01682714 American_chameleon | n01693334 green_lizard\n",
      "433 | n03773504 missile | n04008634 projectile\n",
      "417 | n03710637 maillot | n03710721 maillot\n",
      "416 | n01753488 horned_viper | n01756291 sidewinder\n",
      "410 | n12144580 corn | n13133613 ear\n",
      "406 | n04505470 typewriter_keyboard | n04264628 space_bar\n",
      "399 | n04355933 sunglass | n04356056 sunglasses\n",
      "389 | n02110185 Siberian_husky | n02109961 Eskimo_dog\n",
      "370 | n02979186 cassette_player | n04392985 tape_player\n",
      "369 | n04356056 sunglasses | n04355933 sunglass\n",
      "367 | n02109961 Eskimo_dog | n02110185 Siberian_husky\n",
      "364 | n01797886 ruffed_grouse | n01807496 partridge\n",
      "360 | n02412080 ram | n02415577 bighorn\n",
      "360 | n13133613 ear | n12144580 corn\n",
      "354 | n02395406 hog | n02396427 wild_boar\n",
      "354 | n03642806 laptop | n03832673 notebook\n",
      "351 | n03146219 cuirass | n02895154 breastplate\n",
      "351 | n01773797 garden_spider | n01773157 black_and_gold_garden_spider\n",
      "346 | n02229544 cricket | n02226429 grasshopper\n",
      "342 | n04090263 rifle | n02749479 assault_rifle\n",
      "332 | n03710721 maillot | n03710637 maillot\n",
      "330 | n02123159 tiger_cat | n02123045 tabby\n",
      "326 | n02113624 toy_poodle | n02113712 miniature_poodle\n",
      "317 | n02113712 miniature_poodle | n02113624 toy_poodle\n",
      "314 | n02106030 collie | n02106166 Border_collie\n",
      "302 | n02808440 bathtub | n04493381 tub\n"
     ]
    }
   ],
   "source": [
    "for i,j in largest_non_diag[:30]:\n",
    "    print(int(round(cj[i,j])), \"|\", idx2nid[i], idx2name[i], \"|\",  idx2nid[j], idx2name[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 row sums in confident joint (most noisy classes):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('n04560804', 'water_jug'),\n",
       " ('n03710637', 'maillot'),\n",
       " ('n04392985', 'tape_player'),\n",
       " ('n09332890', 'lakeside'),\n",
       " ('n03180011', 'desktop_computer'),\n",
       " ('n02808440', 'bathtub'),\n",
       " ('n03832673', 'notebook'),\n",
       " ('n04041544', 'radio'),\n",
       " ('n09428293', 'seashore'),\n",
       " ('n03179701', 'desk'),\n",
       " ('n03773504', 'missile'),\n",
       " ('n02123045', 'tabby'),\n",
       " ('n01756291', 'sidewinder'),\n",
       " ('n04026417', 'purse'),\n",
       " ('n07579787', 'plate'),\n",
       " ('n04356056', 'sunglasses'),\n",
       " ('n02109961', 'Eskimo_dog'),\n",
       " ('n03976657', 'pole'),\n",
       " ('n03782006', 'monitor'),\n",
       " ('n04008634', 'projectile'),\n",
       " ('n03866082', 'overskirt'),\n",
       " ('n03871628', 'packet'),\n",
       " ('n03532672', 'hook'),\n",
       " ('n04152593', 'screen'),\n",
       " ('n01693334', 'green_lizard'),\n",
       " ('n01740131', 'night_snake'),\n",
       " ('n04355933', 'sunglass'),\n",
       " ('n03216828', 'dock'),\n",
       " ('n07930864', 'cup'),\n",
       " ('n02988304', 'CD_player')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 30 row sums in confident joint (most noisy classes):\\n\")\n",
    "[(idx2nid[i], idx2name[i]) for i in np.argsort(cj_non_diag.sum(axis = 0))[::-1][:30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analye the validation set on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO CHANGE EXPERIMENT\n",
    "# pyx_file = 'imagenet_val_out.npy' # NO FINE TUNING\n",
    "pyx_file = 'imagenet_val_out_cv_10fold.npy' # fine tuned with 10fold cv\n",
    "\n",
    "# where imagenet dataset is located\n",
    "val_dir = '/datasets/datasets/imagenet/val/'\n",
    "# Stored results directory\n",
    "pyx_dir = '/datasets/cgn/pyx/imagenet/'\n",
    "\n",
    "# Load in data\n",
    "with open(pyx_dir + 'imagenet_val_out_cv_10fold.npy', 'rb') as f:\n",
    "    out = np.load(f)\n",
    "with open(pyx_dir + 'imagenet_val_labels.npy', 'rb') as f:\n",
    "    labels = np.load(f)\n",
    "pyx = torch.nn.functional.softmax(torch.from_numpy(out), dim = 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up mapping for imagenet validation data\n",
    "url = 'https://gist.githubusercontent.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57/'\n",
    "url += 'raw/aa66dd9dbf6b56649fa3fab83659b2acbf3cbfd1/map_clsloc.txt'\n",
    "with urlopen(url) as f:\n",
    "    lines = [x.decode('utf-8') for x in f.readlines()]    \n",
    "    nid2name = dict([(l.split(\" \")[0], l.split(\" \")[2][:-1]) for l in lines])\n",
    "    \n",
    "dataset = datasets.ImageFolder(val_dir)\n",
    "nid2idx = dataset.class_to_idx\n",
    "idx2nid = {v: k for k, v in nid2idx.items()}\n",
    "name2nid = {v: k for k, v in nid2name.items()}\n",
    "idx2name = {k: nid2name[v] for k, v in idx2nid.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj = cleanlab.latent_estimation.estimate_confident_joint_from_probabilities(labels, pyx)\n",
    "py, nm, inv = cleanlab.latent_estimation.estimate_latent(cj, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_non_diag = cj - np.eye(len(cj)) * cj.diagonal()\n",
    "largest_non_diag_raveled = np.argsort(cj_non_diag.ravel())[::-1]\n",
    "largest_non_diag = np.unravel_index(largest_non_diag_raveled, cj_non_diag.shape)\n",
    "largest_non_diag = list(zip(*(list(z) for z in largest_non_diag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 row sums in confident joint (most noisy classes):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('n02123159', 'tiger_cat'),\n",
       " ('n04355933', 'sunglass'),\n",
       " ('n04152593', 'screen'),\n",
       " ('n02113624', 'toy_poodle'),\n",
       " ('n02979186', 'cassette_player'),\n",
       " ('n02808440', 'bathtub'),\n",
       " ('n03642806', 'laptop'),\n",
       " ('n03637318', 'lampshade'),\n",
       " ('n02988304', 'CD_player'),\n",
       " ('n02107908', 'Appenzeller'),\n",
       " ('n02871525', 'bookshop'),\n",
       " ('n01753488', 'horned_viper'),\n",
       " ('n01682714', 'American_chameleon'),\n",
       " ('n03180011', 'desktop_computer'),\n",
       " ('n02110185', 'Siberian_husky'),\n",
       " ('n02412080', 'ram'),\n",
       " ('n03710637', 'maillot'),\n",
       " ('n12144580', 'corn'),\n",
       " ('n02441942', 'weasel'),\n",
       " ('n07734744', 'mushroom'),\n",
       " ('n03179701', 'desk'),\n",
       " ('n01740131', 'night_snake'),\n",
       " ('n02106030', 'collie'),\n",
       " ('n01667778', 'terrapin'),\n",
       " ('n04008634', 'projectile'),\n",
       " ('n03950228', 'pitcher'),\n",
       " ('n02119022', 'red_fox'),\n",
       " ('n03710721', 'maillot'),\n",
       " ('n03782006', 'monitor'),\n",
       " ('n04560804', 'water_jug')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 30 row sums in confident joint (most noisy classes):\\n\")\n",
    "[(idx2nid[i], idx2name[i]) for i in np.argsort(cj_non_diag.sum(axis = 1))[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644.983461962514 | n04008634 projectile | n03773504 missile\n",
      "538.9486260454004 | n04493381 tub | n02808440 bathtub\n",
      "475.6818181818183 | n02895154 breastplate | n03146219 cuirass\n",
      "436.84210526315803 | n01693334 green_lizard | n01682714 American_chameleon\n",
      "435.0343473994112 | n01682714 American_chameleon | n01693334 green_lizard\n",
      "432.89205702647666 | n03773504 missile | n04008634 projectile\n",
      "417.0893054024257 | n03710637 maillot | n03710721 maillot\n",
      "415.62819203268646 | n01753488 horned_viper | n01756291 sidewinder\n",
      "410.09463722397487 | n12144580 corn | n13133613 ear\n",
      "406.25000000000006 | n04505470 typewriter_keyboard | n04264628 space_bar\n",
      "399.1764705882354 | n04355933 sunglass | n04356056 sunglasses\n",
      "389.451476793249 | n02110185 Siberian_husky | n02109961 Eskimo_dog\n",
      "369.8744769874478 | n02979186 cassette_player | n04392985 tape_player\n",
      "368.79432624113485 | n04356056 sunglasses | n04355933 sunglass\n",
      "366.8049792531121 | n02109961 Eskimo_dog | n02110185 Siberian_husky\n",
      "363.6563876651983 | n01797886 ruffed_grouse | n01807496 partridge\n",
      "359.6340150699678 | n02412080 ram | n02415577 bighorn\n",
      "359.51903807615236 | n13133613 ear | n12144580 corn\n",
      "353.98335315101076 | n02395406 hog | n02396427 wild_boar\n",
      "353.96039603960406 | n03642806 laptop | n03832673 notebook\n",
      "351.0659898477158 | n03146219 cuirass | n02895154 breastplate\n",
      "350.64935064935077 | n01773797 garden_spider | n01773157 black_and_gold_garden_spider\n",
      "345.89800443458984 | n02229544 cricket | n02226429 grasshopper\n",
      "341.68704156479225 | n04090263 rifle | n02749479 assault_rifle\n",
      "332.3108384458078 | n03710721 maillot | n03710637 maillot\n",
      "330.1587301587302 | n02123159 tiger_cat | n02123045 tabby\n",
      "326.0869565217391 | n02113624 toy_poodle | n02113712 miniature_poodle\n",
      "316.52173913043487 | n02113712 miniature_poodle | n02113624 toy_poodle\n",
      "313.69565217391306 | n02106030 collie | n02106166 Border_collie\n",
      "301.96292257360966 | n02808440 bathtub | n04493381 tub\n"
     ]
    }
   ],
   "source": [
    "for i,j in largest_non_diag[:30]:\n",
    "    print(cj[i,j], \"|\", idx2nid[i], idx2name[i], \"|\",  idx2nid[j], idx2name[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
